{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea15c11-ddab-45f6-91b9-e30a4d768ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, lit, monotonically_increasing_id, when\n",
    "from pyspark.sql.types import DateType, StructType, StructField, StringType, IntegerType, LongType, FloatType, BooleanType, ArrayType\n",
    "from datetime import datetime\n",
    "import re\n",
    "from sklearn.impute import KNNImputer\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab1513b-e962-4708-a16d-b90c43d60378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.result = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        mean = np.mean(X, axis=0)\n",
    "        centered_X = X - mean\n",
    "        cov_mat = np.cov(centered_X, rowvar=False, bias=True)\n",
    "        w, v = np.linalg.eig(cov_mat)\n",
    "        idx = np.argsort(w)[::-1][:self.k]\n",
    "        w = w[idx]\n",
    "        v = v[:, idx]\n",
    "        v = -v\n",
    "        self.result = v\n",
    "        res = 'PCA(n_components=' + str(self.k) + ')'\n",
    "\n",
    "        return res\n",
    "\n",
    "    def transform(self, X):\n",
    "        mean = np.mean(X, axis=0)\n",
    "        centered_X = X - mean\n",
    "        transformed_X = np.matmul(centered_X, self.result)\n",
    "\n",
    "        return transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685ebe40-7e8c-492a-8568-3592ff35a4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self, lr = 0.001, n_iters = 1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights, self.bias = None, None\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        takes in results of the linear prediction and returns the sigmoid\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        takes in the dataset values as X and the classification as y, and updates the coefficients\n",
    "        \"\"\"\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            probability = self._sigmoid(linear_pred)\n",
    "            \n",
    "            # calculationg error\n",
    "            dw = (1/X.shape[0]) * np.dot(X.T, (probability - y))\n",
    "            db = (1 / X.shape[0]) * (np.sum(probability - y))\n",
    "\n",
    "            # updating weights and bias\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        takes in the dataset values and returns probablity of predictions\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        prediction = self._sigmoid(linear_pred)\n",
    "        return prediction\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        takes in the dataset values and threshold, and returns 1 or 0 based on probablity prediction and the threshold\n",
    "        \"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return [1 if i > threshold else 0 for i in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f78459c-4aec-4265-a94c-56453f1ecf5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_column(df, col_vals, fields, cols_to_drop = None):\n",
    "    \"\"\"\n",
    "    takes in a list of column's values and a schema as a list and adds the column to the dataframe\n",
    "    \"\"\"\n",
    "    final_fields = [StructField(\"id\", IntegerType(), True)]\n",
    "    # prepares the new column(s) and drops the old one\n",
    "    for field in fields:\n",
    "        col_name = field[0]\n",
    "        col_type = field[1]\n",
    "        if cols_to_drop is not None:\n",
    "            if type(cols_to_drop) is list:\n",
    "                for col_to_drop in cols_to_drop:\n",
    "                    df = df.drop(col_to_drop)\n",
    "            else:\n",
    "                df = df.drop(cols_to_drop)\n",
    "        else:\n",
    "            df = df.drop(col_name)\n",
    "        final_fields.append(StructField(col_name, col_type, True))\n",
    "    \n",
    "    schema = StructType(fields=final_fields)\n",
    "    temp_df = spark.createDataFrame(col_vals, schema)\n",
    "    df = df.join(temp_df, on=[\"id\"], how=\"inner\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ae7fc4-bf50-4d13-ab55-e69e61ed9818",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_binary_cols(df, og_col, cols):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        df - a spark dataframe\n",
    "        og_col - a name of a column as string\n",
    "        cols - a list of names of columns we want to add (values from og_col)\n",
    "    output:\n",
    "        a new dataframe without og_col, and with columns that are in cols with binary values. (1 - if the name of the column in og_col, else 0)\n",
    "    \"\"\"\n",
    "    fields = [] # for the schema\n",
    "    col_numbers_dict = {} # dictionary for storing columns indexes, the keys are the column name and the values are the indexes\n",
    "    j = 1\n",
    "    for col_name in cols: # each value gets an index\n",
    "        col_numbers_dict[col_name] = j\n",
    "        col_name_with_og = og_col + '_' + col_name\n",
    "        fields.append([col_name_with_og, IntegerType()])\n",
    "        j+=1\n",
    "\n",
    "    temp_row = [0 for k in range(len(cols))] # a list of binary values\n",
    "    vals_dict = df.select(\"id\", og_col).rdd.collectAsMap() # dictionary of ids as keys and the value of the ogirinal column as values\n",
    "    new_data = [[i, *temp_row] for i in sorted(vals_dict.keys())] # a list of the new rows\n",
    "\n",
    "    for row_index, curr_row in enumerate(new_data):\n",
    "        row_id = curr_row[0] # the row's id\n",
    "        val = vals_dict[row_id] # the row's value\n",
    "        if val not in col_numbers_dict.keys():\n",
    "            continue\n",
    "        col_index = col_numbers_dict[val] # the column's index of the value\n",
    "        new_data[row_index][col_index] = 1\n",
    "\n",
    "    return add_column(df, new_data, fields, og_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7326dbb9-7224-4f83-8b9a-0f53e2111467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def kmeans(df, cols, k):\n",
    "    \"\"\"\n",
    "    grouping houses by location and returns list of indexes of each group (location)\n",
    "    \"\"\"\n",
    "    rdd_df = df.select(\"id\", *cols).rdd\n",
    "    centroids = rdd_df.takeSample(False, k, 2023)\n",
    "    cent_dict = {}\n",
    "    i = 0\n",
    "    def dist(x, y):\n",
    "        \"\"\"\n",
    "        takes in a row and dict of centroids and returns the number of centroid with the smallest distance between them\n",
    "        \"\"\"\n",
    "        tmin = 1000000\n",
    "        final = 0\n",
    "        for num in y.keys():\n",
    "            temp = y[num]\n",
    "            result = 0\n",
    "            for j in range(1,3):\n",
    "                result += (x[j] - temp[j])**2\n",
    "            result = np.sqrt(result)\n",
    "            if result < tmin:\n",
    "                tmin = result\n",
    "                final = num\n",
    "        return final\n",
    "    \n",
    "    def sum_list(x, y):\n",
    "        result = [0, 0, 0]\n",
    "        for i in range(1, len(x)):\n",
    "            result[i] = x[i] + y[i]\n",
    "        return result\n",
    "\n",
    "        # creates a dictionary of centroids\n",
    "    for cent in centroids:\n",
    "        cent_dict[i] = list(cent)\n",
    "        i+=1\n",
    "\n",
    "    rows_by_cents = rdd_df.map(lambda x: [dist(x, cent_dict), list(x)])\n",
    "    for itr in range(10):\n",
    "        sum_rows = rows_by_cents.reduceByKey(lambda x,y: sum_list(x,y)) # sum of the columns of each row of each cluster\n",
    "        count_rows = rows_by_cents.mapValues(lambda x: 1).reduceByKey(lambda x,y: x+y) # counts the number of rows of each cluster\n",
    "        joined = sum_rows.join(count_rows).collect()\n",
    "        joined_rdd=sc.parallelize(joined)\n",
    "        avgs = joined_rdd.mapValues(lambda row: [x/row[1] for x in row[0]]) # contains the number of cluster and the average of the rows\n",
    "        for cent in avgs.collect(): # updates the centroids dictionary\n",
    "            cent_num = cent[0]\n",
    "            cent_value = cent[1]\n",
    "            cent_dict[cent_num] = cent_value\n",
    "        rows_by_cents = rdd_df.map(lambda x: [dist(x, cent_dict), list(x)])\n",
    "    result = []\n",
    "    for row in rows_by_cents.collect(): # creates a list of the id and the new column\n",
    "        result.append([row[1][0],row[0]])\n",
    "                \n",
    "    return cent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19af28c7-5462-4f9f-a3e9-2081ab416f28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def assign_location(df):\n",
    "    \"\"\"\n",
    "    replaces the \"latitude\" and \"longitude\" columns with \"location\" column, where each location gets a number based on classifiction that was made using kmeans\n",
    "    input:\n",
    "        df - a spark dataframe with \"latitude\" and \"longitude\" columns\n",
    "    output:\n",
    "        final_df - a spark dataframe with \"location\" column instead of \"latitude\" and \"longitude\"\n",
    "    \"\"\"\n",
    "    cent_dict = {0: [0.0, 35.64644872143282, 139.64445828059533], 1: [0.0, 35.7012928578391, 139.6874795486678], 2: [0.0, 35.72620743526037, 139.8375019727198], 3: [0.0, 35.71471221537529, 139.6216737771336], 4: [0.0, 35.69828154103825, 139.71176844587723], 5: [0.0, 35.574037140065975, 139.7253905209628], 6: [0.0, 35.65121075414842, 139.72129169587166], 7: [0.0, 35.74263885066194, 139.71986369007038], 8: [0.0, 35.70728394607018, 139.7877636192585], 9: [0.0, 35.68004331676238, 139.41011747307735]}\n",
    "    vals_list = df.select(\"id\", \"latitude\", \"longitude\").rdd.flatMap(lambda x: [[x.id, x[\"latitude\"], x[\"longitude\"]]]).collect()\n",
    "    rdd_df = df.select(\"id\", \"latitude\", \"longitude\").rdd\n",
    "    def dist(x, y):\n",
    "        \"\"\"\n",
    "        takes in a row and dict of centroids and returns the number of centroid with the smallest distance between them\n",
    "        \"\"\"\n",
    "        tmin = 1000000\n",
    "        final = 0\n",
    "        for num in y.keys():\n",
    "            temp = y[num]\n",
    "            result = 0\n",
    "            for j in range(1,3):\n",
    "                result += (x[j] - temp[j])**2\n",
    "            result = np.sqrt(result)\n",
    "            if result < tmin:\n",
    "                tmin = result\n",
    "                final = num\n",
    "        return final\n",
    "    rows_by_cents = rdd_df.map(lambda x: [dist(x, cent_dict), x.id])\n",
    "    vals = sorted(rows_by_cents.flatMap(lambda x: [[x[1], x[0]]]).collect())\n",
    "    schema = [['location', StringType()]]\n",
    "    cols_to_drop = [\"latitude\", \"longitude\"]\n",
    "    final_df = add_column(df, vals, schema, cols_to_drop)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca58c767-12fd-4c47-88ac-3be6afd6af0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def percentage_to_number(df, cols):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        df - spark dataframe\n",
    "        cols - list of names(string) of columns to change\n",
    "    output:\n",
    "        spark dataframe with the values of the columns in cols changed to floats\n",
    "    \"\"\"\n",
    "    cols_to_select = [\"id\"] + cols\n",
    "    selected_df = df.select(*cols_to_select)\n",
    "    vals_list = selected_df.rdd.map(lambda row: [row[\"id\"]] + [row[col] for col in cols]).collect()\n",
    "    fields = [[col_name, FloatType()] for col_name in cols]\n",
    "    for i, row in enumerate(vals_list):\n",
    "        for j, val in enumerate(row):\n",
    "            if j == 0:\n",
    "                continue\n",
    "            elif val == \"N/A\" or val is None:\n",
    "                vals_list[i][j] = None\n",
    "            else:\n",
    "                new_val = val.replace(\"%\", \"\")\n",
    "                vals_list[i][j] = float(new_val)\n",
    "    return add_column(df, vals_list, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5586597-3cae-4431-8136-b50eed2b79d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_property_type(df):\n",
    "    \"\"\"\n",
    "    cleans values in the properties column\n",
    "    input:\n",
    "        df - a spark dataframe with a \"properties\" column\n",
    "    output:\n",
    "        a spark dataframe with a cleaned \"properties\" column\n",
    "    \"\"\"\n",
    "    vals_list = df.select(\"id\", \"property_type\").rdd.flatMap(lambda x: [[x.id, x[\"property_type\"]]]).collect()\n",
    "    for i, row in enumerate(vals_list):\n",
    "        val = row[1].lower()\n",
    "        words = val.split(\" \")\n",
    "        if \"in\" in words:\n",
    "            cut_index = words.index(\"in\")\n",
    "            temp = ' '.join(words[cut_index+1:])\n",
    "            vals_list[i][1] = temp\n",
    "        elif \"entire\" in words:\n",
    "            cut_index = words.index(\"entire\")\n",
    "            temp = ' '.join(words[cut_index+1:])\n",
    "            vals_list[i][1] = temp\n",
    "        elif \"private\" in words:\n",
    "            cut_index = words.index(\"private\")\n",
    "            temp = ' '.join(words[cut_index+1:])\n",
    "            vals_list[i][1] = temp\n",
    "        else:\n",
    "            vals_list[i][1] = val\n",
    "    fields = [[\"property_type\", StringType()]]\n",
    "    return add_column(df, vals_list, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f94240eb-49ec-44bd-a9ab-cba01f750b3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def split_bathroom_col(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        spark dataframe\n",
    "    output:\n",
    "        spark dataframe without bathrooms_text column and with 2 more columns (bathrooms_text_private and bathrooms_text_shared)\n",
    "    \"\"\"\n",
    "    fields = [[\"bathrooms_text_private\", FloatType()], [\"bathrooms_text_shared\", FloatType()]] # for the schema\n",
    "    vals_dict = df.select(\"id\", \"bathrooms_text\").rdd.collectAsMap() # dictionary with rows id as keys and bathrooms_text values as values\n",
    "    temp_row = [0.0, 0.0]\n",
    "    # col_numbers_dict = {\"private\": 1, \"shared\": 2} # dictionary that stores columns indexes, the keys are the column name and the values are the indexes\n",
    "    new_data = [[i, *temp_row] for i in sorted(vals_dict.keys())] # a list of the new rows\n",
    "\n",
    "    for row_index, curr_row in enumerate(new_data):\n",
    "        row_id = curr_row[0] # the row's id\n",
    "        val = vals_dict[row_id] # the row's value\n",
    "        if val is None:\n",
    "            continue\n",
    "        words = val.lower().split(\" \")\n",
    "\n",
    "        if \"shared\" in words:\n",
    "            col_index = 2\n",
    "        else:\n",
    "            col_index = 1\n",
    "\n",
    "        if \"half-bath\" in words:\n",
    "            new_data[row_index][col_index] = 0.5\n",
    "        else:\n",
    "            new_data[row_index][col_index] = float(words[0])\n",
    "\n",
    "    return add_column(df, new_data, fields, \"bathrooms_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc577bc4-bc90-4211-8207-ed9d9130a5e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class imputer:\n",
    "    \"\"\"\n",
    "    a class for imputing data in a spark dataframe\n",
    "    \"\"\"\n",
    "    def __init__(self, df, schemas, mode):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            df - spark dataframe\n",
    "            schemas - list for a schema\n",
    "            mode - the way the imputer calculates the new values\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.schemas = schemas\n",
    "        self.col = None\n",
    "        self.coltype = None\n",
    "        self.mode = mode\n",
    "\n",
    "    def _mean(self):\n",
    "        \"\"\"\n",
    "        calculates the mean of a column\n",
    "        \"\"\"\n",
    "        col_name = self.col\n",
    "        col_mean = self.df.select(col_name).filter(col(col_name).isNotNull()).rdd.flatMap(lambda x: [x[col_name]]).mean()\n",
    "        # result = int(col_mean + 0.5) # calculate the mean of the column and rounds it to the closets number\n",
    "        # col_list = [row[self.col] for row in col_list]\n",
    "        # filtered_list = list(filter(None, col_list)) # drop none values\n",
    "        # result = int(sum(filtered_list)/len(filtered_list)+0.5) # calculate the mean of the column and rounds it to the closets number\n",
    "        return col_mean\n",
    "    \n",
    "    def _median(self):\n",
    "        \"\"\"\n",
    "        calculates the median of a column\n",
    "        \"\"\"\n",
    "        # result = self.df.agg(median(self.col)).toPandas().values[0][0]\n",
    "        col = self.col\n",
    "        df = self.df\n",
    "        col_list = df.select(col).rdd.flatMap(lambda x: [x[col]]).collect()\n",
    "        filtered_list = sorted(list(filter(None, col_list))) # drops none values and sorts the list\n",
    "        n = len(filtered_list)\n",
    "        if n % 2 == 0:\n",
    "            mid_index = int(n/2)-1\n",
    "            median = (filtered_list[mid_index]+filtered_list[mid_index+1])/2\n",
    "        else:\n",
    "            mid_index = n//2\n",
    "            median = filtered_list[mid_index]\n",
    "        return median\n",
    "    \n",
    "    def _most_frequent(self):\n",
    "        \"\"\"\n",
    "        returns the most frequent value of a column\n",
    "        \"\"\"\n",
    "        col_list = self.df.select(self.col).rdd.flatMap(lambda x: [x[self.col]]).collect()\n",
    "        frequency_dict = {}\n",
    "        for val in col_list:\n",
    "            if val is None:\n",
    "                continue\n",
    "            elif val not in frequency_dict.keys():\n",
    "                frequency_dict[val] = 1\n",
    "            else:\n",
    "                frequency_dict[val] += 1\n",
    "        most_frequent = max(frequency_dict, key=lambda x: frequency_dict[x])\n",
    "        return most_frequent\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        replaces null values with values based on self.mode\n",
    "        \"\"\"\n",
    "        for schema in self.schemas:\n",
    "            self.col = schema[0]\n",
    "            self.coltype = schema[1]\n",
    "            col_name = self.col\n",
    "            vals_list = self.df.select(\"id\", col_name).rdd.flatMap(lambda x: [[x.id, x[col_name]]]).collect()\n",
    "            if self.mode == \"mean\": # replaces null values with the average of values in the chosen column\n",
    "                if self.coltype == FloatType():\n",
    "                    mean = self._mean()\n",
    "                else:\n",
    "                    mean = int(self._mean()+0.5)\n",
    "                for i, row in enumerate(vals_list):\n",
    "                    val = row[1]\n",
    "                    if val is None:\n",
    "                        vals_list[i][1] = mean\n",
    "            elif self.mode == \"median\": # replaces null values with the median of values in the chosen column\n",
    "                median = self._median()\n",
    "                for i, row in enumerate(vals_list):\n",
    "                    val = row[1]\n",
    "                    if val is None:\n",
    "                        vals_list[i][1] = median\n",
    "            elif self.mode == \"most_frequent\": # replaces null values with the most frequent value of values in the chosen column\n",
    "                most_frequent = self._most_frequent()\n",
    "                for i, row in enumerate(vals_list):\n",
    "                    val = row[1]\n",
    "                    if val is None:\n",
    "                        vals_list[i][1] = most_frequent\n",
    "            else:\n",
    "                for i, row in enumerate(vals_list):\n",
    "                    val = row[1]\n",
    "                    if val is None:\n",
    "                        vals_list[i][1] = self.mode\n",
    "            self.df = add_column(self.df, vals_list, [schema])\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30e1d5c-6b97-4c89-8b26-762cff85d244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_amenities(amenities):\n",
    "    \"\"\"\n",
    "    takes in amenities column's value and returns it clean\n",
    "    \"\"\"\n",
    "    p1= r'\\\\u\\w+'\n",
    "    p2 = r'\\w+\\\\'\n",
    "    pattern = f'({p1}|{p2})'\n",
    "    new_amenities = amenities[1:-1].replace('\"', '').replace('  ', ' ').lower()\n",
    "    new_amenities = re.sub(pattern, '', new_amenities)\n",
    "    new_amenities = new_amenities.split(\", \")\n",
    "    return new_amenities\n",
    "\n",
    "def amenities_cols(df):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        df - spark dataframe\n",
    "    output:\n",
    "        spark dataframe without the column 'amenities' and with specific amenities binary columns\n",
    "    \"\"\"\n",
    "    amenities = [\"private entrance\", \"ethernet connection\", \"body soap\", \"bidet\", \"shower gel\", \"rice maker\", \"conditioner\", \"bathtub\", \"dining table\", \"freezer\", \"room-darkening shades\", \"elevator\"]\n",
    "    fields = [[f'amenities_{col_name.replace(\"-\",\"_\").replace(\" \", \"_\")}', IntegerType()] for col_name in amenities]\n",
    "    number_of_amenities = len(amenities)\n",
    "    temp_row = [0 for _ in range(number_of_amenities)]\n",
    "    amenities_list = df.select(\"id\",\"amenities\").rdd.flatMap(lambda x: [[x.id, clean_amenities(x.amenities)]]).collect()\n",
    "    new_data = [[row[0], *temp_row] for row in amenities_list]\n",
    "    col_index_dict = {} # each column gets an index\n",
    "    j = 1\n",
    "    for amenity in amenities:\n",
    "        col_index_dict[amenity] = j\n",
    "        j+=1\n",
    "    \n",
    "    for row_index, row in enumerate(amenities_list):\n",
    "        row_id = row[0]\n",
    "        row_amenities = row[1]\n",
    "        for col in amenities:\n",
    "            if col in row_amenities:\n",
    "                col_index = col_index_dict[col]\n",
    "                new_data[row_index][col_index] = 1\n",
    "\n",
    "    return add_column(df, new_data, fields, 'amenities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca17635a-f86d-496e-91c1-2a311e29d610",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        takes in a spark dataframe\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "\n",
    "    def analysis_transform(self):\n",
    "        \"\"\"\n",
    "        returns a processed spark dataframe\n",
    "        \"\"\"\n",
    "        self.df = clean_property_type(self.df)\n",
    "            \n",
    "\n",
    "        # change the \"latitude\" and \"longitude\" columns to \"location\" based on kmeans distribution\n",
    "        self.df = assign_location(self.df)\n",
    "\n",
    "        self.df = split_bathroom_col(self.df)\n",
    "        \n",
    "\n",
    "        def days_passed(date):\n",
    "            \"\"\"\n",
    "            takes in date and returns the number of days that passed since it\n",
    "            \"\"\"\n",
    "            if date is None:\n",
    "                return date\n",
    "            else:\n",
    "                date_val = datetime.strptime(date, \"%d/%m/%Y\")\n",
    "                current_date = datetime.now()\n",
    "                time_difference = current_date - date_val\n",
    "                days_passed = time_difference.days\n",
    "                return days_passed\n",
    "                \n",
    "            # changing the values of date columns to the number of days that passed since the date\n",
    "\n",
    "        date_cols = [\"host_since\", \"last_review\", \"first_review\"]\n",
    "        new_data = self.df.select(\"id\", *date_cols).rdd.flatMap(lambda x: [[x.id, days_passed(x[\"host_since\"]), days_passed(x[\"last_review\"]), days_passed(x[\"first_review\"])]]).collect()\n",
    "        schema = [[col, IntegerType()] for col in date_cols]\n",
    "        self.df = add_column(self.df, new_data, schema)\n",
    "                \n",
    "        # turning percentage columns into floats\n",
    "\n",
    "        percentage_cols = [\"host_response_rate\", \"host_acceptance_rate\"]\n",
    "        self.df = percentage_to_number(self.df, percentage_cols)\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    def train_transform(self):\n",
    "        \"\"\"\n",
    "        returns a processed spark dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        binary_cols = [['host_is_superhost',['f', 't']], ['host_identity_verified',['f', 't']]]\n",
    "        for col in binary_cols:\n",
    "            og_col = col[0]\n",
    "            new_cols = col[1]\n",
    "            self.df = add_binary_cols(self.df, og_col, new_cols)\n",
    "        # turning amenities with most likely chance to effect the price into binary columns\n",
    "\n",
    "        # split \"location\" column to binary cols in order to avoid logistic regression thinking there is hierarchy\n",
    "\n",
    "        self.df = add_binary_cols(self.df, 'location', [str(num) for num in range(10)])\n",
    "\n",
    "        self.df = amenities_cols(self.df)\n",
    "\n",
    "\n",
    "        schemas = [['review_scores_rating', FloatType()], ['review_scores_accuracy', FloatType()], ['review_scores_cleanliness', FloatType()], ['review_scores_value', FloatType()]]\n",
    "        mean_imputer = imputer(self.df, schemas, \"mean\")\n",
    "        self.df = mean_imputer.transform()\n",
    "\n",
    "        selected_columns = [['review_scores_checkin', FloatType()], ['review_scores_communication', FloatType()], ['review_scores_location', FloatType()]]\n",
    "        median_imputer = imputer(self.df, schemas, \"median\")\n",
    "        self.df = median_imputer.transform()\n",
    "\n",
    "        columns_to_drop = ['property_type','room_type','host_has_profile_pic','bathrooms_text','host_response_rate', 'host_acceptance_rate', 'amenities', 'host_id','host_response_time','host_verifications','longitude', 'latitude', 'minimum_nights','maximum_nights', 'minimum_minimum_nights','maximum_minimum_nights', 'minimum_maximum_nights', 'minimum_nights_avg_ntm','maximum_nights_avg_ntm','has_availability','availability_30', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d', 'license', 'instant_bookable', 'reviews_per_month', 'bathrooms_text_shared', 'bathrooms_text_private', 'maximum_maximum_nights']\n",
    "\n",
    "        self.df = self.df.drop(*columns_to_drop)\n",
    "        self.df = self.df.drop('id')\n",
    "\n",
    "\n",
    "        column_names = self.df.columns\n",
    "        pandas_df = self.df.toPandas()\n",
    "        df_values = pandas_df.values.tolist()\n",
    "        knnimputer = KNNImputer(n_neighbors = 5)\n",
    "        imputed_data = knnimputer.fit_transform(df_values)\n",
    "        self.df = spark.createDataFrame(imputed_data.tolist(), column_names)\n",
    "        return self.df\n",
    "    \n",
    "    def test_transform(self):\n",
    "        \"\"\"\n",
    "        returns a processed spark dataframe\n",
    "        \"\"\"\n",
    "        binary_cols = [['host_is_superhost',['f', 't']], ['host_identity_verified',['f', 't']]]\n",
    "        for col in binary_cols:\n",
    "            og_col = col[0]\n",
    "            new_cols = col[1]\n",
    "            self.df = add_binary_cols(self.df, og_col, new_cols)\n",
    "        # turning amenities with most likely chance to effect the price into binary columns\n",
    "\n",
    "        # split \"location\" column to binary cols in order to avoid logistic regression thinking there is hierarchy\n",
    "\n",
    "        self.df = add_binary_cols(self.df, 'location', [str(num) for num in range(10)])\n",
    "\n",
    "        self.df = amenities_cols(self.df)\n",
    "        \n",
    "\n",
    "\n",
    "        schemas = [['review_scores_rating', FloatType()], ['review_scores_accuracy', FloatType()], ['review_scores_cleanliness', FloatType()], ['review_scores_value', FloatType()]]\n",
    "        mean_imputer = imputer(self.df, schemas, \"mean\")\n",
    "        self.df = mean_imputer.transform()\n",
    "\n",
    "        selected_columns = [['review_scores_checkin', FloatType()], ['review_scores_communication', FloatType()], ['review_scores_location', FloatType()]]\n",
    "        median_imputer = imputer(self.df, schemas, \"median\")\n",
    "        self.df = median_imputer.transform()\n",
    "\n",
    "        columns_to_drop = ['property_type','room_type','host_has_profile_pic','bathrooms_text','host_response_rate', 'host_acceptance_rate', 'amenities','host_id','host_response_time',  'host_verifications','longitude', 'latitude', 'minimum_nights','maximum_nights', 'minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'minimum_nights_avg_ntm','maximum_nights_avg_ntm','has_availability','availability_30', 'availability_60', 'availability_90', 'availability_365', 'number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d', 'license', 'instant_bookable', 'reviews_per_month', 'bathrooms_text_shared', 'bathrooms_text_private', 'maximum_maximum_nights']\n",
    "\n",
    "        self.df = self.df.drop(*columns_to_drop)\n",
    "        self.df = self.df.drop('id')\n",
    "\n",
    "\n",
    "        column_names = self.df.columns\n",
    "        pandas_df = self.df.toPandas()\n",
    "        df_values = pandas_df.values.tolist()\n",
    "        knnimputer = KNNImputer(n_neighbors = 5)\n",
    "        imputed_data = knnimputer.fit_transform(df_values)\n",
    "        self.df = spark.createDataFrame(imputed_data.tolist(), column_names)\n",
    "\n",
    "        return self.df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "classes and functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
