from pyspark.sql.functions import regexp_replace, col, lit, monotonically_increasing_id, when
from pyspark.sql.types import DateType, StructType, StructField, StringType, IntegerType, LongType, FloatType, BooleanType, ArrayType
from datetime import datetime
import re
from sklearn.impute import KNNImputer
from pyspark.ml.feature import StringIndexer, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd

class preprocessing:
    def __init__(self, df):
        // """
        // takes in a spark dataframe
        // """
        self.df = df

    def indexer(self, col):
        // """
        // takes in a column name and returns the values of the column as numbers
        // """
        col_list = self.df.select("id", col).rdd.flatMap(lambda x: [[x.id, x[col]]]).collect()
        temp_dict = {}
        index = 0
        for i, row in enumerate(col_list):
            item = row[1]
            // // if the value was empty, keep it empty
            if item == "N/A" or item is None:
                col_list[i][1] = None
            // // if the value isnt in our dictionary - add it and change the value to an index
            elif item not in temp_dict.keys():
                temp_dict[item] = index
                col_list[i][1] = index
                index += 1
            // // if the value is in our dictionary - change the value to an index
            else:
                col_list[i][1] = temp_dict[item]
        return col_list
    
    def add_column(self, col_vals, schema):
        // """
        // takes in a list of column's values and a schema as a list and adds the column to the dataframe
        // """
         // takes in a list of column's values
        col_name = schema[0]
        col_type = schema[1]
        self.df = self.df.drop(col_name)
        schema = StructType(fields=[
            StructField("id", IntegerType(), True),
            StructField(col_name, col_type, True)])
        temp_df = spark.createDataFrame(col_vals, schema)
        self.df = self.df.join(temp_df, on=["id"], how="inner")
    
    def clean_amenities(self, amenities):
        // takes in amenities column's value and returns it cleaned
        p1= r'\\u\w+'
        p2 = r'\w+\\'
        pattern = f'({p1}|{p2})'
        new_amenities = amenities[1:-1].replace('"', '').replace('  ', ' ').lower()
        new_amenities = re.sub(pattern, '', new_amenities)
        return new_amenities
    
    def fit_amenities(self, amenity):
        // """
        // given amenity, returns a list of binary values that indicates if each house has that amenity.
        // """
        amenities_list = self.df.select("id","amenities").rdd.flatMap(lambda x: [[x.id, x.amenities]]).collect()
        new_vals = []
        for row in amenities_list:
            amenities = row[1]
            index = row[0]
            cleaned_amenities = self.clean_amenities(amenities)
            if amenity in cleaned_amenities:
                new_row = [index, 1]
            else:
                new_row = [index, 0]
            new_vals.append(new_row)
        return new_vals
    
    def binary_indexer(self, col):
        // """
        // takes in a boolean column and returns a list of binary values.
        // """
        vals_list = self.df.select("id", col).rdd.flatMap(lambda x: [[x.id, x[col]]]).collect()
        for i, row in enumerate(vals_list):
            val = row[1]
            if val == 'f':
                vals_list[i][1] = 0
            elif val == 't':
                vals_list[i][1] = 1
            else:
                vals_list[i][1] = None
        return vals_list

    def percentage_to_number(self, col):
        // """
        // takes in a column with percentage values and returns a list of integers
        // """
        vals_list = self.df.select("id", col).rdd.flatMap(lambda x: [[x.id, x[col]]]).collect()
        for i, row in enumerate(vals_list):
            val = row[1]
            if val == "N/A" or val is None:
                vals_list[i][1] = None
            else:
                vals_list[i][1] = int(val[:-1])
        return vals_list

    def kmeans(self):
        // """
        // grouping houses by location and returns list of indexes of each group (location)
        // """
        rdd_df = self.df.select("id", "latitude", "longitude").rdd
        centroids = rdd_df.takeSample(False, 10)
        cent_dict = {}
        i = 0
        def dist(x, y):
            // """
            // takes in a row and dict of centroids and returns the number of centroid with the smallest distance between them
            // """
            tmin = 1000000
            final = 0
            for num in y.keys():
                temp = y[num]
                result = 0
                for j in range(1,3):
                    result += (x[j] - temp[j])**2
                result = np.sqrt(result)
                if result < tmin:
                    tmin = result
                    final = num
            return final

        def sum_list(x, y):
            result = [0, 0, 0]
            for i in range(1, len(x)):
                result[i] = x[i] + y[i]
            return result

        // // creates a dictionary of centroids
        for cent in centroids:
            cent_dict[i] = list(cent)
            i+=1

        rows_by_cents = rdd_df.map(lambda x: [dist(x, cent_dict), list(x)])
        for itr in range(10):
            sum_rows = rows_by_cents.reduceByKey(lambda x,y: sum_list(x,y)) // sum of the columns of each row of each cluster
            count_rows = rows_by_cents.mapValues(lambda x: 1).reduceByKey(lambda x,y: x+y) // counts the number of rows of each cluster
            joined = sum_rows.join(count_rows).collect()
            joined_rdd=sc.parallelize(joined)
            avgs = joined_rdd.mapValues(lambda row: [x/row[1] for x in row[0]]) // contains the number of cluster and the average of the rows
            for cent in avgs.collect(): // updates the centroids dictionary
                cent_num = cent[0]
                cent_value = cent[1]
                cent_dict[cent_num] = cent_value
            rows_by_cents = rdd_df.map(lambda x: [dist(x, cent_dict), list(x)])
        result = []
        for row in rows_by_cents.collect(): // creates a list of the id and the new column
            result.append([row[1][0],row[0]])
                
        return result

    def fit(self):
        // """
        // returns a processed spark dataframe
        // """
        // indexer
        cols_to_replace = ["host_response_time", "property_type", "room_type", "bathrooms_text"]
        for col in cols_to_replace:
            new_col = self.indexer(col)
            schema = [col, IntegerType()]
            self.add_column(new_col, schema)
        
        // turning amenities with most likely chance to effect the price into binary columns

        amenities_to_keep = ["private entrance", "ethernet connection", "body soap", "bidet", "shower gel", "rice maker", "conditioner", "bathtub", "dining table", "freezer", "room-darkening shades", "elevator"]

        for amenity in amenities_to_keep:
            col_vals = self.fit_amenities(amenity)
            schema = [amenity.replace(" ", "_").replace("-", "_"), IntegerType()]
            self.add_column(col_vals, schema)
        
        // turning date columns into number of days passed since that date
        date_cols = ["host_since", "first_review", "last_review"]

        def days_passed(date):
            """
            takes in date and returns the number of days that passed since it
            """
            if date is None:
                return date
            else:
                date_val = datetime.strptime(date, "%d/%m/%Y")
                current_date = datetime.now()
                time_difference = current_date - date_val
                days_passed = time_difference.days
                return days_passed
            
        for col in date_cols:
            new_col = self.df.select("id", col).rdd.flatMap(lambda x: [[x.id, days_passed(x[col])]]).collect()
            schema = [col, IntegerType()]
            self.add_column(new_col, schema)
        
        // changing boolean columns into binary
        boolean_cols = ["host_is_superhost", "host_has_profile_pic", "host_identity_verified", "has_availability", "instant_bookable"]
        for col in boolean_cols:
            col_vals = self.binary_indexer(col)
            schema = [col, IntegerType()]
            self.add_column(col_vals, schema)
        
        percentage_cols = ["host_response_rate", "host_acceptance_rate"]
        for col in percentage_cols:
            col_vals = self.percentage_to_number(col)
            schema = [col, IntegerType()]
            self.add_column(col_vals, schema)
        
        // location
        col_vals = self.kmeans()
        schema = ["location", IntegerType()]
        self.add_column(col_vals, schema)

        // drops columns

        self.df = self.df.drop("id","host_id", "host_verifications", "amenities", "license", "longitude", "latitude")

        // fills null values

        column_names = self.df.columns
        pandas_df = self.df.toPandas()
        df_values = pandas_df.values.tolist()
        imputer = KNNImputer(n_neighbors = 10)
        imputed_data = imputer.fit_transform(df_values)
        self.df = spark.createDataFrame(imputed_data.tolist(), column_names)

        return self.df


class PCA:
    def __init__(self, k):
        self.k = k
        self.result = None

    def fit(self, X):
        mean = np.mean(X, axis=0)
        centered_X = X - mean
        cov_mat = np.cov(centered_X, rowvar=False, bias=True)
        w, v = np.linalg.eig(cov_mat)
        idx = np.argsort(w)[::-1][:self.k]
        w = w[idx]
        v = v[:, idx]
        v = -v
        self.result = v
        res = 'PCA(n_components=' + str(self.k) + ')'

        return res

    def transform(self, X):
        mean = np.mean(X, axis=0)
        centered_X = X - mean
        transformed_X = np.matmul(centered_X, self.result)

        return transformed_X
